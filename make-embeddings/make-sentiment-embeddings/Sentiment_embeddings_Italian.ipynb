{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_1 = joblib.load(open(\"predictions_tablets_IT.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_2 = joblib.load(open(\"predictions_automobiles_IT.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablets_automobiles = [\"tablets_IT_extra_data\",\"automobiles_IT_extra_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting comments from:  /media/moniek/Elements/Thesis/Make_normal_Embeddings/tablets_IT_extra_data\n",
      "collecting comments from:  /media/moniek/Elements/Thesis/Make_normal_Embeddings/automobiles_IT_extra_data\n"
     ]
    }
   ],
   "source": [
    "raw_data_list = []\n",
    "\n",
    "for dirfile in tablets_automobiles:\n",
    "        print(\"collecting comments from: \",dirfile)    \n",
    "        for filename in os.listdir(dirfile):\n",
    "            file = dirfile+\"/\"+filename\n",
    "            if file[-4:] == \"json\":\n",
    "                with open(file) as json_data:\n",
    "                    d = json.load(json_data)\n",
    "                    for comment in d[\"comments\"]:\n",
    "                        lower = comment[\"text\"].lower().strip()\n",
    "                        raw_data_list.append([comment[\"id\"],d[\"video_id\"],lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99328\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99328\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212632\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create commentlists of positive, negative and neutral comments\n",
    "\n",
    "labellist_pos_test = []\n",
    "commentlist_pos_test =[]\n",
    "labellist_neg_test = []\n",
    "commentlist_neg_test =[]\n",
    "labellist_neu_test = []\n",
    "commentlist_neu_test =[]\n",
    "for comment in raw_data_list:\n",
    "    commentid, videoid_raw, comment_text = comment\n",
    "    \n",
    "    if commentid in predictiondict:\n",
    "        \n",
    "        label,videoid,comment_output = predictiondict[commentid]\n",
    "        \n",
    "        if videoid == videoid_raw:\n",
    "            \n",
    "            raw_comment = comment_text\n",
    "            comment = raw_comment.lower().strip()\n",
    "            words = comment.split()\n",
    "            \n",
    "            if label == \"positive\":\n",
    "                commentlist_pos_test.append(words)\n",
    "                labellist_pos_test.append(label)\n",
    "            elif label == \"negative\":\n",
    "                commentlist_neg_test.append(words)\n",
    "                labellist_neg_test.append(label)\n",
    "            elif label == \"neutral\":\n",
    "                commentlist_neu_test.append(words)\n",
    "                labellist_neu_test.append(label)\n",
    "        else:\n",
    "            print(commentid, videoid_raw, comment_text )\n",
    "    else:\n",
    "        print(commentid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24516\n",
      "9516\n",
      "183570\n",
      "217602\n"
     ]
    }
   ],
   "source": [
    "print(len(commentlist_pos_test))\n",
    "print(len(commentlist_neg_test))\n",
    "print(len(commentlist_neu_test))\n",
    "\n",
    "print(len(commentlist_pos_test)+len(commentlist_neg_test)+len(commentlist_neu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames =[\"lemma\",\"POS\",\"WordnetID\",\"PosScore\",\"NegScore\",\"polarity\",\"intensity\"]\n",
    "lexiconfile = pd.read_csv('sentix', sep=\"\\t\", header=None, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>WordnetID</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "      <th>polarity</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abile</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>intelligente</td>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lemma POS  WordnetID  PosScore  NegScore  polarity  intensity\n",
       "0         abile   a       1740     0.125       0.0       1.0      0.125\n",
       "1  intelligente   a       1740     0.125       0.0       1.0      0.125"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexiconfile.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1858 4367\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "\n",
    "for num, row in lexiconfile.iterrows():\n",
    "    # add positives\n",
    "    if row['PosScore'] > 0.6 and row['NegScore'] < 0.2:\n",
    "        words = row['lemma'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                pos.append(word)\n",
    "    # add negatives\n",
    "    elif row['NegScore'] > 0.6 and row['PosScore'] < 0.2:\n",
    "        words = row['lemma'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                neg.append(word)\n",
    "\n",
    "print(len(pos), len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if words occur in both lists, if they do check how many times\n",
    "# if word occur more in one list remove in other, if equal remove from both lists\n",
    "\n",
    "\n",
    "new_pos = []\n",
    "new_neg = []\n",
    "\n",
    "for word in pos:\n",
    "    if word in neg:\n",
    "        if pos.count(word) > neg.count(word):\n",
    "            if word not in new_pos:\n",
    "                new_pos.append(word)\n",
    "        elif neg.count(word) > pos.count(word):\n",
    "            if word not in new_neg:\n",
    "                new_neg.append(word)\n",
    "    else:\n",
    "        if word not in new_pos:\n",
    "            new_pos.append(word)\n",
    "\n",
    "for word in neg:\n",
    "    if word not in new_neg and word not in pos:\n",
    "        new_neg.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245 2765\n"
     ]
    }
   ],
   "source": [
    "print(len(new_pos),len(new_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ieri', 'mattina', 'a', 'mio', 'figlio', 'di', '9anni', 'è', 'caduto', 'il', 'clempad', '4.4', '6-12', 'anni', 'regalato', 'da', 'babbo', 'n.il', 'dispay', 'esternamente', 'non', 'si', 'è', 'rotto', 'ma_pos', 'si', 'vede', 'una', 'riga', 'interna', 'e', 'il', 'video', 'ora', 'è', 'a', 'metà,neanche', 'il', 'tempo_pos', 'di', 'vedere', 'come_pos', 'funziona.come', 'posso', 'fare...(lui', 'è', 'disperato!!!!!)', 'vado', 'dal', 'rivenditore', 'oppure', 'mi', 'può', 'consigliare', 'come_pos', 'muovermi', 'per_pos', 'una', 'riparazione', 'più', 'rapida', 'possibile,grazie.andrea'], ['bellissima', 'recensione!!!!!', 'utile', 'e', 'completa.....è', 'stato', 'un', 'piacere_pos', 'guardarla', 'aspetto', \"anch'io\", 'quella', 'del', 'clempad', '4.4', 'xl', '6-12', 'anni....utile', 'anche', \"l'informazione\", 'sulla', 'scheda', 'sd.....alla', 'prossima!!!!!', ';-)'], ['aspetto', 'la', 'recensione', 'del', 'clempad', 'xl'], ['ahahhahaha', 'grande_pos', 'patrix!!', '=)'], ['bellissima', 'davvero!', 'io', 'uso', 'la', 'porta', 'obd', 'per_pos', 'lavoro', 'ma_pos', 'non', 'avevo', 'mai', 'pensato', 'che', 'si', 'potesse', 'usare', 'anche', 'in', 'questo', 'modo', 'connessa', 'allo', 'smartphone.', 'interessantissimo,', 'grazie!', ':d'], ['video', 'qualitativamente', 'pessimo...mi', 'è', 'venuto', 'il', 'mal', 'di', 'mare...'], ['davvero_pos', 'fantastico_pos', '!!!'], ['#cacagoo', 'grande_pos', 'ale!'], ['io', 'ho', 'ipad', 'mini', 'senza', 'display', 'retina', 'e', 'mi', 'trovo', 'benissimo'], ['io', \"l'ho\", 'acquistato', 'con', 'wifi', 'cell', 'ed.....veramente', 'figo_pos', 'ed', 'leggerissimo', ':-)', 'conplimenti', 'per_pos', 'la', 'apple', ':-)']]\n",
      "38803\n"
     ]
    }
   ],
   "source": [
    "# Add _pos to positive words\n",
    "\n",
    "new_pos_comments = []\n",
    "changed_words_pos = []\n",
    "\n",
    "for line in commentlist_pos_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_pos:\n",
    "            \n",
    "            new_word = word +\"_pos\"\n",
    "            \n",
    "#             count_changes_pos[word] += 1\n",
    "\n",
    "            new_line.append(new_word)\n",
    "            changed_words_pos.append(word)\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    new_pos_comments.append(new_line)\n",
    "\n",
    "print(new_pos_comments[:10])\n",
    "print(len(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_pos = Counter(changed_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['io', 'ho', 'cancellato', \"l'errore\", 'avaria', 'motore', 'dovuto', 'alla', 'valvola', 'egr', '(dopo', 'averla', 'pulita)', 'con', 'questo', 'trasmettitore...risparmiato', 'un', 'sacchissimo', 'di', 'soldi...'], ['bellissimo', 'accessorio_neg', 'ma', 'fate', 'attenzione', 'soprattutto', 'pe', 'quelle', 'auto', 'ancora', 'in', 'garanzia', 'del', 'costruttore,', 'se', 'disgraziatamente_neg', 'rimanete', 'a', 'piedi', 'e', 'la', 'colpa_neg', 'è', 'di', 'questo', 'accessorio_neg', 'pagate', 'voi.', 'ocio'], ['ma', 'la', 'mia', 'meriva', 'del', '2012', 'ha', 'la', 'porta', 'obd?', 'e', 'simpatico', 'sto', 'coso'], ['quanta', 'energia', 'in', 'quel', 'puntocom', 'finale!', 'grande', 'gale'], ['e', 'cosi', 'tutti', 'quelli', 'che', 'faranno', 'la', 'scansione', 'dei', 'codici', \"d'errore\", 'si', 'spaventeranno', 'e', 'porteranno', 'la', 'macchina', 'dal', 'meccanico', 'pensando', 'che', 'abbia', 'qualcosa', 'che', 'non_neg', 'va', 'hahahahah'], ['video', 'molto', '\"ondulato\"', 'ahah', 'un', \"po'\", 'di', 'mal', 'di', 'mare', 'è', 'venuto'], ['io', 'ho', 'provato_neg', 'questo', 'dispositivo', 'e', 'ne', 'sono', 'rimasto', 'veramente', 'soddisfatto', 'io', 'sono', 'un', 'appassionato', 'di', 'tecnologia.ho', 'provato_neg', 'a', 'smontarlo', 'vedendo', 'complessivamente', 'quello', 'che', \"c'era\", 'dentro', \"(l'ho\", 'comprato', 'rotto', 'su', 'ebay)', 'io', 'non_neg', 'posso', 'permettermelo', 'anche', 'perche', 'come', 'al', 'solito', 'i', 'prezzi', 'sono', 'alti', 'pero', 'a', 'mio', 'parere_neg', 'i', 'soldi', 'spesi', 'in', 'questo', 'dispositivo', 'cosi', 'potente', 'ci', 'valgono', 'molto', 'cosa', 'che', 'non_neg', 'accade', 'con', 'iphone', 'secondo', 'me', 'dovrebbero', 'costare', 'di', 'meno', 'di', 'un', 'dispositivo', 'piu', 'grande.', 'cmq', 'se', 'qualcuno', 'possiedete', 'un', 'ipad', '4', 'non_neg', 'consiglio', \"l'acquisto\", \"dell'air\", 'perchè', 'ho', 'avuto', 'modo', 'di', 'provarli', 'e', 'ce', 'qualche', 'piccola', 'differenza', 'la', 'stessa', 'cosa', 'se', 'possedete', 'un', 'ipad', '3', 'con', 'ios', '6', 'o', '5', 'non_neg', 'installate', 'assolutamente', 'il', '7', 'perchè', 'è', 'pieno', 'di', 'bug', 'oppure', 'se', 'volete', 'assolutamente', 'ios', '7', 'vi', 'consiglio', 'di', 'acquistare', 'un', 'mini', 'retina.', 'per', 'i', 'possessori', 'di', 'ipad', '2', 'potrei', 'consigliarvi', \"l'acquisto\", 'di', 'prendere', \"l'air\", 'ma', 'meglio', 'aspettare', 'il', '6.'], ['ho', 'preso', 'un', 'mini', 'retina', 'al', 'posto', 'del', 'mio', 'notebook...', 'un', 'disastro', 'totale,', 'non_neg', 'per', 'l', 'ipad', 'che', 'è', 'bellissimo', 'ma', 'perché', 'nessun', 'tablet', 'neanche_neg', 'l', 'air', 'o', 'galaxy', 'tab', 'possono', 'sostituire', 'un', 'pc', 'sia', 'fisso', 'che', 'portatile'], ['non_neg', 'è', 'molto', 'facile', 'occorre', 'allenarsi', 'molto', 'nelle', 'tabulazioni!'], ['ma', 'per', 'favore!', 'allo', 'stesso', 'prezzo', 'si', 'ha', 'un', 'nexus', '7', 'con', 'un', 'miglior', 'processore', 'e', 'doppio', 'lo', 'storage', 'interno!']]\n",
      "11754\n"
     ]
    }
   ],
   "source": [
    "# Add _neg to positive words\n",
    "\n",
    "new_neg_comments = []\n",
    "changed_words_neg = []\n",
    "count_changes_neg = 0\n",
    "\n",
    "for line in commentlist_neg_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_neg:\n",
    "            new_word = word +\"_neg\"\n",
    "            count_changes_neg += 1\n",
    "            new_line.append(new_word)\n",
    "            changed_words_neg.append(word)\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    \n",
    "    new_neg_comments.append(new_line)\n",
    "\n",
    "print(new_neg_comments[:10])\n",
    "print(len(changed_words_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg = Counter(changed_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove double words\n",
    "words_neg = list(OrderedDict.fromkeys(changed_words_neg))\n",
    "words_pos = list(OrderedDict.fromkeys(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "print(len(words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words_neg, open(\"changed_words_neg_ALL_IT.p\", \"wb\" ))\n",
    "pickle.dump(words_pos, open(\"changed_words_pos_ALL_IT.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_neg_comments + new_pos_comments + commentlist_neu_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of times each changed word occurs in other context for weighting the embeddings\n",
    "\n",
    "changed_word_neg_other_context = [] \n",
    "changed_word_pos_other_context = [] \n",
    "\n",
    "for line in new_data:\n",
    "    for word in line:\n",
    "        if word in words_neg:\n",
    "            changed_word_neg_other_context.append(word)\n",
    "        elif word in words_pos:\n",
    "            changed_word_pos_other_context.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg_other = Counter(changed_word_neg_other_context)\n",
    "count_changes_pos_other = Counter(changed_word_pos_other_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightdict(count_changes_pos,count_changes_pos_other):\n",
    "    pos_word_weights = {}\n",
    "    for word in count_changes_pos:\n",
    "        pos_count = count_changes_pos[word]\n",
    "        other_count = count_changes_pos_other[word]\n",
    "        total_count_pos = pos_count + other_count\n",
    "        weight_pos = pos_count / total_count_pos\n",
    "        weight_other_pos = other_count / total_count_pos\n",
    "        pos_word_weights[word] = [weight_pos,weight_other_pos]\n",
    "    return pos_word_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_weights = getWeightdict(count_changes_pos,count_changes_pos_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word_weights = getWeightdict(count_changes_neg,count_changes_neg_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['io', 'ho', 'cancellato', \"l'errore\", 'avaria', 'motore', 'dovuto', 'alla', 'valvola', 'egr', '(dopo', 'averla', 'pulita)', 'con', 'questo', 'trasmettitore...risparmiato', 'un', 'sacchissimo', 'di', 'soldi...'], ['bellissimo', 'accessorio', 'ma', 'fate', 'attenzione', 'soprattutto', 'pe', 'quelle', 'auto', 'ancora', 'in', 'garanzia', 'del', 'costruttore,', 'se', 'disgraziatamente', 'rimanete', 'a', 'piedi', 'e', 'la', 'colpa', 'è', 'di', 'questo', 'accessorio', 'pagate', 'voi.', 'ocio'], ['ma', 'la', 'mia', 'meriva', 'del', '2012', 'ha', 'la', 'porta', 'obd?', 'e', 'simpatico', 'sto', 'coso'], ['quanta', 'energia', 'in', 'quel', 'puntocom', 'finale!', 'grande', 'gale'], ['e', 'cosi', 'tutti', 'quelli', 'che', 'faranno', 'la', 'scansione', 'dei', 'codici', \"d'errore\", 'si', 'spaventeranno', 'e', 'porteranno', 'la', 'macchina', 'dal', 'meccanico', 'pensando', 'che', 'abbia', 'qualcosa', 'che', 'non', 'va', 'hahahahah'], ['video', 'molto', '\"ondulato\"', 'ahah', 'un', \"po'\", 'di', 'mal', 'di', 'mare', 'è', 'venuto'], ['io', 'ho', 'provato', 'questo', 'dispositivo', 'e', 'ne', 'sono', 'rimasto', 'veramente', 'soddisfatto', 'io', 'sono', 'un', 'appassionato', 'di', 'tecnologia.ho', 'provato', 'a', 'smontarlo', 'vedendo', 'complessivamente', 'quello', 'che', \"c'era\", 'dentro', \"(l'ho\", 'comprato', 'rotto', 'su', 'ebay)', 'io', 'non', 'posso', 'permettermelo', 'anche', 'perche', 'come', 'al', 'solito', 'i', 'prezzi', 'sono', 'alti', 'pero', 'a', 'mio', 'parere', 'i', 'soldi', 'spesi', 'in', 'questo', 'dispositivo', 'cosi', 'potente', 'ci', 'valgono', 'molto', 'cosa', 'che', 'non', 'accade', 'con', 'iphone', 'secondo', 'me', 'dovrebbero', 'costare', 'di', 'meno', 'di', 'un', 'dispositivo', 'piu', 'grande.', 'cmq', 'se', 'qualcuno', 'possiedete', 'un', 'ipad', '4', 'non', 'consiglio', \"l'acquisto\", \"dell'air\", 'perchè', 'ho', 'avuto', 'modo', 'di', 'provarli', 'e', 'ce', 'qualche', 'piccola', 'differenza', 'la', 'stessa', 'cosa', 'se', 'possedete', 'un', 'ipad', '3', 'con', 'ios', '6', 'o', '5', 'non', 'installate', 'assolutamente', 'il', '7', 'perchè', 'è', 'pieno', 'di', 'bug', 'oppure', 'se', 'volete', 'assolutamente', 'ios', '7', 'vi', 'consiglio', 'di', 'acquistare', 'un', 'mini', 'retina.', 'per', 'i', 'possessori', 'di', 'ipad', '2', 'potrei', 'consigliarvi', \"l'acquisto\", 'di', 'prendere', \"l'air\", 'ma', 'meglio', 'aspettare', 'il', '6.'], ['ho', 'preso', 'un', 'mini', 'retina', 'al', 'posto', 'del', 'mio', 'notebook...', 'un', 'disastro', 'totale,', 'non', 'per', 'l', 'ipad', 'che', 'è', 'bellissimo', 'ma', 'perché', 'nessun', 'tablet', 'neanche', 'l', 'air', 'o', 'galaxy', 'tab', 'possono', 'sostituire', 'un', 'pc', 'sia', 'fisso', 'che', 'portatile'], ['non', 'è', 'molto', 'facile', 'occorre', 'allenarsi', 'molto', 'nelle', 'tabulazioni!'], ['ma', 'per', 'favore!', 'allo', 'stesso', 'prezzo', 'si', 'ha', 'un', 'nexus', '7', 'con', 'un', 'miglior', 'processore', 'e', 'doppio', 'lo', 'storage', 'interno!']]\n"
     ]
    }
   ],
   "source": [
    "print(commentlist_neg_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217602\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both skip models\n",
    "\n",
    "# model_SKIP = Word2Vec(new_data, size=300, sg=1)\n",
    "model_SKIP_n10 = Word2Vec(new_data, size=300, sg=1, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('stupendo_pos', 0.8124523162841797),\n",
       " ('spettacolare!', 0.8005304932594299),\n",
       " ('simpatico_pos', 0.7953038215637207),\n",
       " ('fantastico', 0.7924375534057617),\n",
       " ('divertentissimo', 0.7803273797035217),\n",
       " ('favoloso', 0.7680243253707886),\n",
       " ('carino_pos', 0.7674349546432495),\n",
       " ('bellissimo!', 0.7673802375793457),\n",
       " ('grandeeee', 0.7631446123123169),\n",
       " ('fantastico!', 0.7628900408744812)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SKIP_n10.most_similar(\"fantastico_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32531"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 55976\n",
    "# 115221\n",
    "model_SKIP_n10.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SKIP_n10.save(\"SKIP_negative10_IT_Change_inputSentiment_no_delete_test.model\")\n",
    "# model_SKIP.save(\"SKIP_IT_Change_inputSentiment_no_delete_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32531\n",
      "\n",
      "587\n",
      "345\n",
      "31599\n",
      "\n",
      "32531\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Delete other context words\n",
    "\n",
    "model_dict_delete = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP_n10.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        if non_word not in model_dict_delete:\n",
    "            model_dict_delete[non_word] = model_SKIP_n10.wv[word]\n",
    "        else:\n",
    "            print(word)\n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_delete:\n",
    "                model_dict_delete[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31944\n"
     ]
    }
   ],
   "source": [
    "print(len(model_dict_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.89118081e-01 -3.79209787e-01 -1.96773708e-01 -5.39195761e-02\n",
      " -3.05712014e-01  3.67750637e-02  3.77034873e-01  4.96099412e-01\n",
      "  3.72668020e-02  2.08207574e-02  2.94383578e-02 -3.34917873e-01\n",
      " -1.35159222e-02 -1.54792219e-01  2.40416452e-01 -8.47082511e-02\n",
      " -1.76645085e-01  2.66523138e-02 -2.33833808e-02  1.07569769e-02\n",
      " -1.23223104e-01 -1.06809728e-01 -8.88472274e-02  2.82694101e-01\n",
      " -8.40720385e-02 -1.27765313e-01 -1.42776310e-01  1.52525127e-01\n",
      " -2.33185112e-01  3.40788543e-01  6.08045831e-02 -4.95503247e-02\n",
      "  3.18533480e-01 -6.33776858e-02  3.37480873e-01  1.57377794e-01\n",
      " -8.49978402e-02  2.16086075e-01  1.34160668e-01  9.11097899e-02\n",
      "  5.09929717e-01  7.27943331e-02 -1.87426582e-01  1.40792102e-01\n",
      "  1.23210721e-01 -2.98863743e-02 -5.39682843e-02 -1.10212363e-01\n",
      "  2.01096326e-01  2.63399214e-01  1.58809528e-01  3.90238464e-02\n",
      "  3.06568444e-01  5.47487587e-02  1.17591456e-01  1.37621969e-01\n",
      " -7.25888759e-02 -1.97256535e-01 -1.33607060e-01  8.90514553e-02\n",
      "  1.08468220e-01 -6.68198243e-03  4.46730703e-02 -2.56487370e-01\n",
      " -7.87046775e-02  2.35028967e-01  4.75864783e-02 -2.26994023e-01\n",
      " -5.33825438e-03  2.80832406e-03  1.34121897e-02 -8.37329701e-02\n",
      "  1.76549092e-01  4.02565807e-01  8.01278129e-02 -1.60403520e-01\n",
      " -6.19012080e-02 -1.65484190e-01 -8.12639967e-02  2.34153301e-01\n",
      " -1.15954444e-01 -1.13761559e-01 -1.04765989e-01 -1.19335175e-01\n",
      "  1.46796122e-01 -4.77170318e-01  3.37468758e-02 -1.41006052e-01\n",
      "  5.13222478e-02 -2.04896346e-01  2.08707348e-01  3.02297417e-02\n",
      " -1.72190357e-03 -1.85270965e-01  4.92749736e-02  1.53184775e-02\n",
      "  1.09106965e-01  1.25288656e-02 -9.92874578e-02 -1.38883710e-01\n",
      " -2.01257557e-01  1.46422729e-01 -1.23739026e-01 -3.57372127e-02\n",
      " -1.31406143e-01  9.54833813e-03  4.51119803e-02  2.83907324e-01\n",
      "  2.49867767e-01 -1.43575534e-01  2.11566284e-01  2.17526287e-01\n",
      " -3.32710475e-01  2.22667605e-01 -2.01568063e-02 -1.35257572e-01\n",
      "  1.42361417e-01  1.14452615e-01 -7.19326586e-02 -1.70745458e-02\n",
      "  2.94168681e-01  4.85375933e-02 -5.00126146e-02  9.38480813e-03\n",
      " -1.77011296e-01  3.54042649e-01  1.14487313e-01  1.49654984e-01\n",
      "  2.69011617e-01  1.24815360e-01  1.63596705e-01 -9.46175903e-02\n",
      " -3.77752185e-02 -9.35405716e-02 -2.82153517e-01  2.50814520e-02\n",
      "  1.62700459e-01 -9.47180912e-02  2.99888164e-01 -1.78558767e-01\n",
      " -1.05773218e-01 -3.10285389e-02 -4.78979014e-02  7.12399855e-02\n",
      "  5.84958419e-02  1.14178091e-01 -1.04595922e-01 -1.21094778e-01\n",
      " -1.97997630e-01  3.38215232e-01  3.94412167e-02 -4.16352361e-01\n",
      "  9.91311818e-02  4.37185019e-02 -5.01324311e-02 -3.51077169e-02\n",
      "  2.80506045e-01 -1.62593260e-01  2.56186038e-01 -2.04282343e-01\n",
      " -6.23401515e-02 -1.80133104e-01  1.10045485e-01 -4.12534960e-02\n",
      " -1.49284840e-01  1.43587508e-03  2.87182868e-01  1.82028692e-02\n",
      "  1.11507155e-01 -2.18129039e-01  2.13034637e-02 -2.00950965e-01\n",
      " -2.52012789e-01  1.79467872e-01 -1.12966232e-01  1.24082111e-01\n",
      "  2.35877439e-01  1.25385776e-01  7.03982934e-02 -6.42879829e-02\n",
      " -1.51966706e-01  8.50594565e-02  1.50548682e-01 -2.91414410e-01\n",
      "  5.48647754e-02 -1.00489028e-01 -9.18248668e-02  1.50190562e-01\n",
      " -1.40748024e-01 -1.68237716e-01 -4.26210873e-02  1.05996646e-01\n",
      "  1.94411516e-01 -1.25404760e-01  3.79163452e-04  2.49116533e-02\n",
      "  1.06897622e-01  1.00571312e-01 -2.04169512e-01  3.02046575e-02\n",
      " -1.27873346e-01 -1.06995687e-01 -2.73302019e-01 -1.83290377e-01\n",
      " -9.55219343e-02 -9.44921672e-02  1.57240719e-01 -1.89425826e-01\n",
      " -1.92108423e-01  1.22850470e-01  1.15469068e-01 -1.26229331e-01\n",
      "  5.05434811e-01  1.32029086e-01 -7.80213252e-02  1.44382253e-01\n",
      "  1.50821105e-01  2.80347884e-01 -2.17106342e-01 -2.30105082e-03\n",
      " -5.68760149e-02  2.18289699e-02  1.90302357e-01 -7.59162381e-02\n",
      "  3.43260653e-02  2.55261417e-02 -1.42003745e-01  1.92342222e-01\n",
      " -1.00178674e-01  1.80571213e-01  1.09093092e-01 -2.12234780e-01\n",
      "  5.45706786e-02 -2.28972197e-01 -1.97705328e-01 -9.61275622e-02\n",
      " -1.77971736e-01  1.69564590e-01 -1.47014126e-01  1.06192425e-01\n",
      "  1.45704662e-02  2.74908125e-01 -3.21694940e-01 -6.32902011e-02\n",
      "  1.23260401e-01  2.18370706e-01 -1.15479976e-01 -3.80419865e-02\n",
      " -9.42472070e-02  8.79601091e-02  3.26999545e-01  1.15304977e-01\n",
      " -1.21072508e-01  6.98505118e-02 -1.98151022e-02 -8.57188255e-02\n",
      "  3.68866146e-01  1.65689737e-02 -6.54115900e-02  7.83963874e-02\n",
      "  4.03876044e-02  9.61549231e-04  1.94285437e-01 -1.33253068e-01\n",
      "  4.80790317e-01 -2.78923899e-01  1.26697704e-01  6.17439635e-02\n",
      "  6.84968829e-02 -1.67805508e-01 -7.22318655e-03 -1.02535874e-01\n",
      " -2.18810931e-01  6.00762554e-02  1.98415807e-03  2.56429613e-01\n",
      "  1.42001688e-01 -3.59864265e-01 -1.73146322e-01 -1.45233423e-01\n",
      "  2.51639575e-01  6.21574521e-02 -1.22212045e-01  8.23258683e-02\n",
      " -1.24699913e-01 -1.75406277e-01 -4.05151546e-02  3.52542549e-01\n",
      "  2.86704183e-01 -1.45600624e-02 -4.37424555e-02 -9.95519832e-02\n",
      "  1.45821035e-01  1.57323614e-01  9.80238840e-02 -1.02804832e-01\n",
      "  2.73161978e-02 -6.44176304e-02 -2.14677587e-01  1.42914340e-01]\n"
     ]
    }
   ],
   "source": [
    "print(model_dict_delete[\"fantastico\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_delete, open(\"SKIP_negative10_IT_Change_inputSentiment_deletedoubles.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32531\n",
      "\n",
      "587\n",
      "345\n",
      "31599\n",
      "\n",
      "32531\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Average the embeddings\n",
    "\n",
    "model_dict_average = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP_n10.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            averagevector = np.average(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[averagevector]\n",
    "            \n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_average:\n",
    "                model_dict_average[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_average, open(\"SKIP_negative10_IT_Change_inputSentiment_average.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32531\n",
      "\n",
      "587\n",
      "345\n",
      "31599\n",
      "\n",
      "32531\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Weight and sum embeddings\n",
    "\n",
    "model_dict_weightsum = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "for word in model_SKIP_n10.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\": \n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        if word[-4:] == \"_neg\":\n",
    "            sentiment_weight, other_weight = neg_word_weights[non_word]\n",
    "        else:\n",
    "            sentiment_weight, other_weight = pos_word_weights[non_word]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector*sentiment_weight)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector*other_weight)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            weight_sum_vector = np.sum(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[weight_sum_vector]\n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_weightsum:\n",
    "                model_dict_weightsum[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_weightsum, open(\"SKIP_negative10_IT_Change_inputSentiment_weightsum.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
