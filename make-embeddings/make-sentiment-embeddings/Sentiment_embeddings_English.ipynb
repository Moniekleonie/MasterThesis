{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_1 = joblib.load(open(\"labeled_data/predictions_tablets_EN.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_2 = joblib.load(open(\"labeled_data/predictions_automobiles_EN.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablets_automobiles = [\"tablets_EN_extra_data\",\"automobiles_EN_extra_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting comments from:  D:/Thesis/Make_normal_Embeddings/tablets_EN_extra_data\n",
      "collecting comments from:  D:/Thesis/Make_normal_Embeddings/automobiles_EN_extra_data\n"
     ]
    }
   ],
   "source": [
    "raw_data_list = []\n",
    "\n",
    "for dirfile in tablets_automobiles:\n",
    "        print(\"collecting comments from: \",dirfile)    \n",
    "        for filename in os.listdir(dirfile):\n",
    "            file = dirfile+\"/\"+filename\n",
    "            if file[-4:] == \"json\":\n",
    "                with open(file) as json_data:\n",
    "                    d = json.load(json_data)\n",
    "                    for comment in d[\"comments\"]:\n",
    "                        lower = comment[\"text\"].lower().strip()\n",
    "                        raw_data_list.append([comment[\"id\"],d[\"video_id\"],lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028136\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028136\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610828\n"
     ]
    }
   ],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create commentlists of positive, negative and neutral comments\n",
    "\n",
    "labellist_pos_test = []\n",
    "commentlist_pos_test =[]\n",
    "labellist_neg_test = []\n",
    "commentlist_neg_test =[]\n",
    "labellist_neu_test = []\n",
    "commentlist_neu_test =[]\n",
    "for comment in raw_data_list:\n",
    "    commentid, videoid_raw, comment_text = comment\n",
    "    \n",
    "    if commentid in predictiondict:\n",
    "        \n",
    "        label,videoid,comment_output = predictiondict[commentid]\n",
    "        \n",
    "        if videoid == videoid_raw:\n",
    "            \n",
    "            raw_comment = comment_text\n",
    "            comment = raw_comment.lower().strip()\n",
    "            words = comment.split()\n",
    "            \n",
    "            if label == \"positive\":\n",
    "                commentlist_pos_test.append(words)\n",
    "                labellist_pos_test.append(label)\n",
    "            elif label == \"negative\":\n",
    "                commentlist_neg_test.append(words)\n",
    "                labellist_neg_test.append(label)\n",
    "            elif label == \"neutral\":\n",
    "                commentlist_neu_test.append(words)\n",
    "                labellist_neu_test.append(label)\n",
    "        else:\n",
    "            print(commentid, videoid_raw, comment_text )\n",
    "    else:\n",
    "        print(commentid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263118\n",
      "103297\n",
      "1249227\n",
      "1615642\n"
     ]
    }
   ],
   "source": [
    "print(len(commentlist_pos_test))\n",
    "print(len(commentlist_neg_test))\n",
    "print(len(commentlist_neu_test))\n",
    "\n",
    "print(len(commentlist_pos_test)+len(commentlist_neg_test)+len(commentlist_neu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconfile = pd.read_csv('lexicons/SentiWordNet_3.0.0.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "      <th>SynsetTerms</th>\n",
       "      <th>Gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>able#1</td>\n",
       "      <td>(usually followed by `to') having the necessar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>unable#1</td>\n",
       "      <td>(usually followed by `to') not having the nece...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  # POS      ID  PosScore  NegScore SynsetTerms  \\\n",
       "0     a  1740.0     0.125      0.00      able#1   \n",
       "1     a  2098.0     0.000      0.75    unable#1   \n",
       "\n",
       "                                               Gloss  \n",
       "0  (usually followed by `to') having the necessar...  \n",
       "1  (usually followed by `to') not having the nece...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexiconfile.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2310 4878\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "\n",
    "for num, row in lexiconfile.iterrows():\n",
    "    # add positives\n",
    "    if row['PosScore'] > 0.6 and row['NegScore'] < 0.2:\n",
    "        words = row['SynsetTerms'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                pos.append(word)\n",
    "    # add negatives\n",
    "    elif row['NegScore'] > 0.6 and row['PosScore'] < 0.2:\n",
    "        words = row['SynsetTerms'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                neg.append(word)\n",
    "\n",
    "print(len(pos), len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if words occur in both lists, if they do check how many times\n",
    "# if word occur more in one list remove in other, if equal remove from both lists\n",
    "\n",
    "new_pos = []\n",
    "new_neg = []\n",
    "\n",
    "for word in pos:\n",
    "    if word in neg:\n",
    "        if pos.count(word) > neg.count(word):\n",
    "            if word not in new_pos:\n",
    "                new_pos.append(word)\n",
    "        elif neg.count(word) > pos.count(word):\n",
    "            if word not in new_neg:\n",
    "                new_neg.append(word)\n",
    "    else:\n",
    "        if word not in new_pos:\n",
    "            new_pos.append(word)\n",
    "\n",
    "for word in neg:\n",
    "    if word not in new_neg and word not in pos:\n",
    "        new_neg.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962 4113\n"
     ]
    }
   ],
   "source": [
    "print(len(new_pos),len(new_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'have', 'a', 'huion', 'and', 'it', 'is', 'massive!!!!!!', 'i', \"don't\", 'know_pos', 'whether', \"it's\", 'just_pos', 'me', 'cos', \"i'm\", 'a', 'midget', 'and', 'like_pos', '11', 'soooooo', 'lol'], ['what', 'was', 'the', 'one', 'you', 'use', 'normally', 'called?', \"i'd\", 'love_pos', 'a', 'cheapish', 'tablet', 'for', 'christmas'], ['i', 'love_pos', 'you,', 'you', 'are', 'literally', 'favorite', 'youtuber', 'right_pos', 'now', \"everyone's\", 'sooo', 'dramatic', '(except', 'joana', 'ceddia)', 'you', 'make_pos', 'my', 'day', '☺️'], ['i', 'love_pos', 'the', 'way', 'you', 'draw!', \"i'm\", 'getting', 'a', 'drawing', 'tablet', 'tomorrow', 'and', 'thinking', 'about', 'doing', 'animation.', 'any', 'tips?'], ['nice_pos', 'and', 'i', 'have', 'no', 'idea', 'how', 'to', 'do', 'digital', 'art', 'with', 'an', 'art', 'tablet', 'i', 'normally', 'do', 'digital', 'art', 'on', 'my', 'android', 'tablet', 'with', 'my', 'finger', 'and', 'i', 'need', 'to', 'get', 'a', 'stylist', 'for', 'it', 'so', 'yea', 'but', 'my', 'goal', 'in', 'life(in', '8th', 'grade', 'rn', 'so', 'dreaming', 'big)', 'is', 'to', 'work', 'in', 'graphic_pos', 'design', 'after', 'high', 'school', 'and', 'on', 'the', 'side', 'to', 'sell', 'my', 'art', 'and', 'crafts', 'at', 'any', 'place', 'possible_pos', 'so', 'yea', 'i', 'may', 'or', 'may', 'not', 'have', 'a', 'tablet', 'one', 'day'], ['i', 'never', 'use', 'eraser', 'ends', 'on', 'my', 'wacoms.', 'just_pos', 'bought', 'a', 'huion', 'and', \"i'm\", 'excited', 'for', 'tomorrow', 'and', 'if', \"that's\", 'the', 'biggest', 'complaint', \"i'm\", 'sooooo', 'stoked!', \"i'm\", 'a', 'vfx', 'artist', 'of', 'years', 'and', 'used', 'to', 'hitting', '\"e\"', 'while', 'having', 'my', 'right_pos', 'hand', 'on', 'the', 'tablet.', 'great_pos', 'review!'], ['wait.', 'i’m', 'confused.', 'do', 'you', 'actually', 'draw', 'on', 'the', 'tablet?', 'either', 'way,', 'much', '❤️', 'you', 'are', 'really_pos', 'good_pos', 'at', 'drawing!', 'i', 'love_pos', 'your', 'channel!'], ['draw', 'with', 'jazza', 'has', 'even', 'more', 'subscriber', 'than', 'u', 'and', 'even', 'gives', 'an', 'unbiased', 'review...that', 'u', 'and', 'ur', 'channel', 'lacks'], ['thank', 'u', 'for', 'making', 'this', 'video', 'this', 'way', 'i', 'know_pos', 'what', 'to', 'buy', 'instead', 'of', 'wasting', 'time_pos', 'reading', 'reveiws', ';~;', 'i', 'lub', 'you', 'ur', 'as', 'cool', 'as', 'potatoes'], ['omg', 'you', 'use', 'gimp', 'too!!!', 'i', 'think_pos', 'i', 'love_pos', 'you', 'even', 'more', 'now']]\n",
      "505828\n"
     ]
    }
   ],
   "source": [
    "# Add _pos to positive words\n",
    "\n",
    "new_pos_comments = []\n",
    "changed_words_pos = []\n",
    "\n",
    "for line in commentlist_pos_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_pos:\n",
    "            \n",
    "            new_word = word +\"_pos\"\n",
    "            \n",
    "#             count_changes_pos[word] += 1\n",
    "\n",
    "            new_line.append(new_word)\n",
    "            changed_words_pos.append(word)\n",
    "            \n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    new_pos_comments.append(new_line)\n",
    "\n",
    "print(new_pos_comments[:100])\n",
    "print(len(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_pos = Counter(changed_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'piece', 'resonates', 'with', 'me', 'too.', \"it's\", 'very', 'simple', 'melodically,', 'harmonically,', 'and', 'formally', 'which', 'is', 'why', 'i', 'like', 'it.', 'it', 'starts', 'out_neg', 'with', 'a', 'descending', 'dominant', 'to', 'tonic', 'relationship,', 'which', 'in', 'western', 'music', 'is', 'very', 'powerful.', \"it's\", 'the', 'basis', 'for', 'pretty', 'much', 'every', 'piece', 'of', 'music', 'you', 'hear.', 'after', 'that,', \"it's\", 'essentially', 'just', 'a', 'minor_neg', 'arpeggio', 'repeated', 'over', 'a', 'tonic', 'drone', 'with', 'an', 'occasional', 'second', 'scale', 'degree', 'that', 'sticks', 'out_neg', 'from', 'the', 'woodwinds.', 'formally', \"it's\", 'very', 'simple', 'too.', 'it', 'starts', 'off_neg', 'calm,', 'becomes', 'a', 'swirl', 'of', 'repeated', 'arpeggios', 'that', 'almost', 'sound', 'out_neg', 'of', 'sync,', 'then', 'returns', 'to', 'a', 'state', 'of', 'calmness.', 'i', 'almost', 'wish', 'this', 'piece', 'were', 'longer,', 'but', 'i', 'think', 'making', 'it', 'longer', 'would', 'kill', 'some', 'of', 'the', 'magic.', 'brevity', 'works', 'here', 'and', 'it', 'captures', 'that', 'fleeting', 'moment', 'of', 'awareness', 'or', 'understanding.'], ['i', 'think', 'huion', 'make', 'good', 'alternatives', 'tablets', 'and', 'especially', 'for', 'the', 'price', 'they', 'are', 'a', 'bargain', '.', 'wacom', 'still_neg', 'have_neg', 'a', 'more', 'complete', 'driver', 'and', 'some', 'little_neg', 'features', 'that', 'you', \"don't\", 'find', 'in', 'the', 'competitors', 'but', 'the', 'pressure', 'control', 'in', 'some', 'of', 'these', 'huion', 'models', 'feel', 'better', 'than', 'any', 'bamboo', 'or', 'intuos', 'in', 'some', 'cases', 'and', 'the', 'build', 'quality', \"it's\", 'even', 'better', 'than', 'wacom', 'in', 'some', 'models', '.', 'in', 'my', 'opinion_neg', 'if', 'you', 'want_neg', 'to', 'buy', 'a', 'mobile', 'studio', 'pro', ',', 'intuos', 'pro', 'medium', 'or', 'cintiq', '27qhd/cintiq', 'pro', '24/32', 'wacom', \"it's\", 'still_neg', 'worthed', 'the', 'money', 'and', 'you', 'get', 'actually', 'features', 'that', 'no_neg', 'one', 'have_neg', '...', 'but', 'if', 'you', 'want_neg', 'to', 'buy', 'a', 'cintiq', '13hd', '-', 'cintiq', '22hd', '-', 'bamboo', 'or', 'the', 'medium', 'and', 'smaller', 'size', 'monitor/tablets', 'from', 'the', 'cintiq', 'pro', 'line', \"it's\", 'not_neg', 'worthed', 'the', 'money', 'and', 'huion', 'like', 'other_neg', 'competitors', 'make', 'better', 'tablets', 'of', 'the', 'same', 'size', 'or', 'bigger', 'for', 'less', 'with', 'almost', 'the', 'same', 'features', 'and', 'you', 'get', 'also', 'a', 'fantastic', 'customer', 'service', '.', 'this', 'is', 'at', 'least', 'my', 'experience', 'i', 'own', 'and', 'used', 'all', 'these', 'tablets', 'i', 'had', 'the', 'chance', 'to', 'compare', 'them', 'pretty', 'well', '...', 'ps.', 'i', 'never_neg', 'used', 'the', 'eraser', 'on', 'my', 'wacom', 'tablets', 'it', 'feel', 'uncomfortable_neg', 'and', 'un', 'precise', 'sometime_neg', '.'], ['i', 'prefer', 'the', 'huion', 'horizontal', 'stand', 'because', 'my', 'monoprice', 'tablet', 'has', 'a', 'vertical', 'pen', 'stand', 'and', 'i', 'constantly', 'knock_neg', 'over', 'the', 'pen', 'lol.'], ['welll', 'the', 'rubber', 'i', 'wont', 'need', 'cause', 'im', 'looking', 'for', 'an', 'ok', 'drawing', 'tablet', '[poor', 'boy]', 'budget', 'is', '£100'], ['screw', 'art', 'tablets', 'i', 'draw', 'with', 'an', 'ipad'], ['i', 'got', 'this', 'tablet', 'for', 'christmas', 'last_neg', 'year', 'and', \"it's\", 'been', 'sitting_neg', 'in', 'my', 'desk', 'drawer', 'since.', 'my', 'laptop', 'is', 'broken', 'and', \"it's\", 'a', 'super', 'slow', 'icraig', 'laptop.', \"i'm\", 'super', 'excited', 'to', 'try', 'it', 'out_neg', 'but', \"i'm\", 'not_neg', 'sure', 'where', 'to', 'start.'], ['i', 'literally', 'got', 'a', 'ad', 'for', 'wacom', 'tablet', '(wacom', 'intuos', 'i', 'think)', 'during', 'this', 'video', 'wth', 'i', 'think', 'wacom', 'wants', 'me', 'to', 'buy', 'their', 'tablet', 'instead', 'of', 'huion’s!'], ['i', 'never_neg', 'used', 'the', 'eraser', 'tip', 'on', 'my', 'old', 'cintiq.', 'pressing', 'a', 'key', 'on', 'your', 'keyboard', 'takes', 'less', 'time', 'than', 'flipping', 'over', 'the', 'pen.', 'so', 'where', 'exactly', 'is', 'that', 'convenience', 'people', 'talk', 'about?'], ['doing', 'a', 'series', 'on', 'the', 'waicom', 'tablet', 'tutorials', 'would', 'be', 'great.', 'i', 'bought', 'mine', 'several', 'years', 'ago', 'and', 'it', 'is', 'still_neg', 'in', 'the', 'box', 'in', 'pristine', 'condition', 'due', 'to', 'its', 'intimidating', 'ways.'], ['man,', 'i', 'wish', 'i', 'could', 'use', 'a', 'tablet', 'like', 'this.', 'but', 'sadly_neg', 'i', 'am', 'a', 'lefty', 'and', 'i', 'cannot', 'use', 'it', 'without', 'my', 'hand', 'pressing', 'the', 'hot', 'keys.']]\n",
      "214007\n"
     ]
    }
   ],
   "source": [
    "# Add _neg to positive words\n",
    "\n",
    "new_neg_comments = []\n",
    "changed_words_neg = []\n",
    "count_changes_neg = 0\n",
    "\n",
    "for line in commentlist_neg_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_neg:\n",
    "            new_word = word +\"_neg\"\n",
    "            count_changes_neg += 1\n",
    "            new_line.append(new_word)\n",
    "            changed_words_neg.append(word)\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    \n",
    "    new_neg_comments.append(new_line)\n",
    "\n",
    "print(new_neg_comments[:10])\n",
    "print(len(changed_words_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg = Counter(changed_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove double words\n",
    "words_neg = list(OrderedDict.fromkeys(changed_words_neg))\n",
    "words_pos = list(OrderedDict.fromkeys(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n"
     ]
    }
   ],
   "source": [
    "print(len(words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(words_neg, open(\"changed_words_neg_ALL.p\", \"wb\" ))\n",
    "# pickle.dump(words_pos, open(\"changed_words_pos_ALL.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_neg_comments + new_pos_comments + commentlist_neu_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of times each changed word occurs in other context for weighting the embeddings\n",
    "\n",
    "changed_word_neg_other_context = [] \n",
    "changed_word_pos_other_context = [] \n",
    "\n",
    "for line in new_data:\n",
    "    for word in line:\n",
    "        if word in words_neg:\n",
    "            changed_word_neg_other_context.append(word)\n",
    "        elif word in words_pos:\n",
    "            changed_word_pos_other_context.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg_other = Counter(changed_word_neg_other_context)\n",
    "count_changes_pos_other = Counter(changed_word_pos_other_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightdict(count_changes_pos,count_changes_pos_other):\n",
    "    pos_word_weights = {}\n",
    "    for word in count_changes_pos:\n",
    "        pos_count = count_changes_pos[word]\n",
    "        other_count = count_changes_pos_other[word]\n",
    "        total_count_pos = pos_count + other_count\n",
    "        weight_pos = pos_count / total_count_pos\n",
    "        weight_other_pos = other_count / total_count_pos\n",
    "        pos_word_weights[word] = [weight_pos,weight_other_pos]\n",
    "    return pos_word_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_weights = getWeightdict(count_changes_pos,count_changes_pos_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word_weights = getWeightdict(count_changes_neg,count_changes_neg_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both skip models\n",
    "\n",
    "# model_SKIP = Word2Vec(new_data, size=300, sg=1)\n",
    "model_SKIP_n10 = Word2Vec(new_data, size=300, sg=1, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.7516839504241943),\n",
       " ('awesome_pos', 0.7325202822685242),\n",
       " ('great', 0.7270063161849976),\n",
       " ('good_pos', 0.7152231931686401),\n",
       " ('terrific', 0.7093876600265503),\n",
       " ('excellent_pos', 0.6888229846954346),\n",
       " ('amazing_pos', 0.6559268236160278),\n",
       " ('splendid_pos', 0.643805205821991),\n",
       " ('excelent', 0.6415426135063171),\n",
       " ('wonderful_pos', 0.6415295600891113)]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SKIP_n10.most_similar(\"great_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115221"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SKIP_n10.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SKIP_n10.save(\"SKIP_negative10_EN_Change_inputSentiment_no_delete_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115221\n",
      "\n",
      "1942\n",
      "1406\n",
      "111873\n",
      "\n",
      "115221\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Delete other context words\n",
    "\n",
    "model_dict_delete = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        if non_word not in model_dict_delete:\n",
    "            model_dict_delete[non_word] = model_SKIP_n10.wv[word]\n",
    "        else:\n",
    "            print(word)\n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_delete:\n",
    "                model_dict_delete[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113279\n"
     ]
    }
   ],
   "source": [
    "print(len(model_dict_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.59416968e-01  3.22017878e-01  1.24966644e-01 -7.26944255e-03\n",
      "  1.18611142e-01  2.84944832e-01 -3.74072611e-01  1.82498217e-01\n",
      " -1.11310765e-01 -2.04666793e-01  5.33664133e-03 -4.21804249e-01\n",
      " -8.36584121e-02 -1.55637532e-01  7.96585456e-02 -4.77901800e-03\n",
      " -8.92233253e-02 -1.60887465e-01  1.51048526e-01  3.95493299e-01\n",
      " -2.19169870e-01  1.03375562e-01 -1.09550962e-02 -1.94786295e-01\n",
      "  4.10436839e-02  2.37075165e-02  4.59755920e-02 -2.44538523e-02\n",
      "  2.75693871e-02  2.10444123e-01 -2.60958206e-02  2.06231236e-01\n",
      "  2.06388071e-01  7.50712231e-02 -9.67616886e-02 -1.23695638e-02\n",
      "  3.12847614e-01 -1.77890837e-01 -5.69908693e-02  1.46759897e-01\n",
      "  1.72496021e-01 -3.67852822e-02 -1.40052829e-02  7.99474120e-02\n",
      "  1.43799737e-01  6.53669704e-04  1.33343309e-01 -4.57338154e-01\n",
      "  1.50974497e-01 -1.29882336e-01 -2.46428549e-02  1.79461658e-01\n",
      "  1.72829419e-01 -8.70544761e-02 -4.05319408e-02  2.69188076e-01\n",
      "  6.90854117e-02 -1.07866108e-01  8.72672200e-02  4.59691882e-02\n",
      " -1.35516748e-01 -2.90446752e-03 -2.26478741e-01 -1.08625531e-01\n",
      "  4.31776345e-02  3.02845448e-01  1.07791357e-01  4.51743556e-03\n",
      " -3.03286761e-01 -2.84874707e-01 -9.59316269e-02 -8.93398374e-02\n",
      "  1.98243380e-01  2.38201648e-01  4.26730104e-02 -1.61809579e-01\n",
      " -2.30486810e-01  2.52942115e-01  3.46807212e-01  1.72295704e-01\n",
      " -3.60552669e-02 -2.46933592e-03  7.79646933e-02  2.26240322e-01\n",
      " -2.92978019e-01  2.19996899e-01  2.50147015e-01 -4.10686404e-01\n",
      "  4.86903042e-02 -9.90970507e-02  2.48532891e-02 -4.41988632e-02\n",
      " -2.59529412e-01  1.03843510e-01 -1.66965872e-01  1.89971030e-02\n",
      " -1.67895451e-01  1.02602705e-01 -6.84977230e-03 -2.15329796e-01\n",
      " -5.70963062e-02  9.21246335e-02  1.76945269e-01 -6.63695395e-01\n",
      " -3.16412449e-01  3.51591676e-01 -3.18567425e-01 -1.32647855e-02\n",
      " -2.92221904e-01  2.24153940e-02  4.04431075e-01  1.95953641e-02\n",
      " -9.23146904e-02 -3.37044299e-01 -2.86921024e-01 -8.83500353e-02\n",
      "  1.88571543e-01  2.01183975e-01  2.48390004e-01 -5.69481403e-02\n",
      "  5.06791212e-02  3.62682164e-01  2.81431496e-01  1.40171841e-01\n",
      "  3.10621768e-01  1.85990721e-01 -2.69362509e-01  3.44996192e-02\n",
      "  1.08457111e-01  1.02124931e-02 -3.18329111e-02 -1.42827317e-01\n",
      " -1.82042122e-01  1.82946563e-01  3.43432208e-03  3.22813988e-01\n",
      " -1.18881613e-01  4.01645869e-01  5.23106493e-02 -3.02540362e-01\n",
      " -7.41733789e-01 -2.19898358e-01 -1.25108346e-01 -8.03777948e-02\n",
      " -1.29080534e-01  1.10072181e-01 -9.49277729e-02  5.42675704e-02\n",
      " -2.51825184e-01  1.52041269e-02  2.02786043e-01  1.96562901e-01\n",
      "  1.98009595e-01 -2.10427538e-01 -1.58017248e-01 -6.23523295e-02\n",
      "  3.87293659e-02  1.03800006e-01  6.63418397e-02  1.86498865e-01\n",
      "  2.66117025e-02  1.38945848e-01  1.61594123e-01  1.65592469e-02\n",
      " -1.69729024e-01  2.89984405e-01  8.70409086e-02  1.50695490e-02\n",
      " -2.62774646e-01 -7.55662993e-02  4.76982147e-01 -1.55586928e-01\n",
      "  1.46079794e-01  1.13974966e-01  1.36696666e-01  5.10405563e-02\n",
      "  1.75906777e-01  2.01150820e-01 -2.10919470e-01 -2.45896786e-01\n",
      " -8.71501211e-03 -2.26321056e-01  3.87328595e-01 -1.95596084e-01\n",
      "  1.10538967e-01  3.90097260e-01  2.70096838e-01 -1.35620207e-01\n",
      " -1.20853834e-01 -6.99324906e-02 -2.99730003e-01  3.43145460e-01\n",
      "  6.23127460e-01 -2.66439840e-02  3.83115292e-01  2.88841575e-02\n",
      "  1.69733584e-01 -5.02872318e-02  1.27844959e-01  2.58298546e-01\n",
      "  2.16991395e-01 -2.05694646e-01 -3.60839292e-02 -1.84471652e-01\n",
      "  1.16453066e-01  1.31498009e-01  2.56102264e-01  6.67711021e-03\n",
      "  2.10481718e-01  6.60899758e-01  7.16059506e-02 -2.11803228e-01\n",
      " -3.76488656e-01 -1.76277950e-01  2.41121370e-02  1.14301562e-01\n",
      " -1.84582219e-01  1.71289533e-01 -2.75665492e-01 -1.32928401e-01\n",
      " -9.23675299e-03 -7.99403042e-02 -1.93535179e-01 -2.52210110e-01\n",
      "  4.10355404e-02  1.03000298e-01  6.65474012e-02 -3.45287323e-02\n",
      "  1.95390791e-01  6.92696869e-02  1.14416820e-04  1.83103651e-01\n",
      " -2.34486628e-02  1.05541833e-01 -4.56108540e-01 -1.31861001e-01\n",
      "  5.51504549e-03 -4.92529988e-01 -4.93651599e-01  5.83064035e-02\n",
      " -4.28397171e-02 -9.04085208e-03  9.64040309e-02  3.02563589e-02\n",
      " -1.46388456e-01  8.45685080e-02  1.80184022e-01  3.81615646e-02\n",
      " -1.51929557e-01 -7.17193559e-02  9.99783054e-02  6.04317859e-02\n",
      "  3.90549272e-01  3.41792226e-01 -2.26024002e-01  2.58363575e-01\n",
      "  2.87992328e-01 -2.13894010e-01  2.35171374e-02  7.30572194e-02\n",
      "  1.57196708e-02  2.79259145e-01 -2.09537417e-01 -1.04652785e-01\n",
      " -3.46316338e-01 -2.23333791e-01  9.05556679e-02  1.16030179e-01\n",
      " -4.45679314e-02 -4.06002343e-01  1.57168478e-01  9.05325711e-02\n",
      " -5.75791746e-02 -1.44386967e-03  3.50055061e-02 -2.28684008e-01\n",
      " -3.89853157e-02  2.83624619e-01  3.21329057e-01 -3.68797243e-01\n",
      "  3.85106653e-01 -1.36652350e-01  1.47804692e-01 -4.73109037e-01\n",
      " -1.44901305e-01  3.99314798e-02 -1.29269227e-01  4.61478308e-02\n",
      " -6.62035793e-02 -1.72509730e-01 -2.19977558e-01  1.42215952e-01\n",
      " -2.33738139e-01  3.23086292e-01  7.84196079e-01 -3.87757033e-01\n",
      " -5.02219237e-02 -4.39280942e-02  2.91683137e-01  1.31514043e-01]\n"
     ]
    }
   ],
   "source": [
    "print(model_dict_delete[\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_delete, open(\"SKIP_EN_Change_inputSentiment_deletedoubles.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115221\n",
      "\n",
      "1942\n",
      "1406\n",
      "111873\n",
      "\n",
      "115221\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Average the embeddings\n",
    "\n",
    "model_dict_average = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            averagevector = np.average(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[averagevector]\n",
    "            \n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_average:\n",
    "                model_dict_average[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_average, open(\"SKIP_EN_Change_inputSentiment_average.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115221\n",
      "\n",
      "1942\n",
      "1406\n",
      "111873\n",
      "\n",
      "115221\n",
      "\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Weight and sum embeddings\n",
    "\n",
    "model_dict_weightsum = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\": \n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        if word[-4:] == \"_neg\":\n",
    "            sentiment_weight, other_weight = neg_word_weights[non_word]\n",
    "        else:\n",
    "            sentiment_weight, other_weight = pos_word_weights[non_word]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector*sentiment_weight)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector*other_weight)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            weight_sum_vector = np.sum(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[weight_sum_vector]\n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_weightsum:\n",
    "                model_dict_weightsum[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_weightsum, open(\"SKIP_EN_Change_inputSentiment_weightsum.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
