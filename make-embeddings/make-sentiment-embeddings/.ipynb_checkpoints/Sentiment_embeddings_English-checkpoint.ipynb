{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_1 = joblib.load(open(\"labeled_data/predictions_tablets_EN.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict_2 = joblib.load(open(\"labeled_data/predictions_automobiles_EN.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablets_automobiles = [\"tablets_EN_extra_data\",\"automobiles_EN_extra_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_list = []\n",
    "\n",
    "for dirfile in tablets_automobiles:\n",
    "        print(\"collecting comments from: \",dirfile)    \n",
    "        for filename in os.listdir(dirfile):\n",
    "            file = dirfile+\"/\"+filename\n",
    "            if file[-4:] == \"json\":\n",
    "                with open(file) as json_data:\n",
    "                    d = json.load(json_data)\n",
    "                    for comment in d[\"comments\"]:\n",
    "                        lower = comment[\"text\"].lower().strip()\n",
    "                        raw_data_list.append([comment[\"id\"],d[\"video_id\"],lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictiondict_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictiondict.update(predictiondict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictiondict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create commentlists of positive, negative and neutral comments\n",
    "\n",
    "labellist_pos_test = []\n",
    "commentlist_pos_test =[]\n",
    "labellist_neg_test = []\n",
    "commentlist_neg_test =[]\n",
    "labellist_neu_test = []\n",
    "commentlist_neu_test =[]\n",
    "for comment in raw_data_list:\n",
    "    commentid, videoid_raw, comment_text = comment\n",
    "    \n",
    "    if commentid in predictiondict:\n",
    "        \n",
    "        label,videoid,comment_output = predictiondict[commentid]\n",
    "        \n",
    "        if videoid == videoid_raw:\n",
    "            \n",
    "            raw_comment = comment_text\n",
    "            comment = raw_comment.lower().strip()\n",
    "            words = comment.split()\n",
    "            \n",
    "            if label == \"positive\":\n",
    "                commentlist_pos_test.append(words)\n",
    "                labellist_pos_test.append(label)\n",
    "            elif label == \"negative\":\n",
    "                commentlist_neg_test.append(words)\n",
    "                labellist_neg_test.append(label)\n",
    "            elif label == \"neutral\":\n",
    "                commentlist_neu_test.append(words)\n",
    "                labellist_neu_test.append(label)\n",
    "        else:\n",
    "            print(commentid, videoid_raw, comment_text )\n",
    "    else:\n",
    "        print(commentid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(commentlist_pos_test))\n",
    "print(len(commentlist_neg_test))\n",
    "print(len(commentlist_neu_test))\n",
    "\n",
    "print(len(commentlist_pos_test)+len(commentlist_neg_test)+len(commentlist_neu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconfile = pd.read_csv('lexicons/SentiWordNet_3.0.0.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconfile.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "\n",
    "for num, row in lexiconfile.iterrows():\n",
    "    # add positives\n",
    "    if row['PosScore'] > 0.6 and row['NegScore'] < 0.2:\n",
    "        words = row['SynsetTerms'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                pos.append(word)\n",
    "    # add negatives\n",
    "    elif row['NegScore'] > 0.6 and row['PosScore'] < 0.2:\n",
    "        words = row['SynsetTerms'].split()\n",
    "        for word in words: \n",
    "            word = word.rstrip('0123456789#')\n",
    "            if '_' not in word:\n",
    "                neg.append(word)\n",
    "\n",
    "print(len(pos), len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if words occur in both lists, if they do check how many times\n",
    "# if word occur more in one list remove in other, if equal remove from both lists\n",
    "\n",
    "new_pos = []\n",
    "new_neg = []\n",
    "\n",
    "for word in pos:\n",
    "    if word in neg:\n",
    "        if pos.count(word) > neg.count(word):\n",
    "            if word not in new_pos:\n",
    "                new_pos.append(word)\n",
    "        elif neg.count(word) > pos.count(word):\n",
    "            if word not in new_neg:\n",
    "                new_neg.append(word)\n",
    "    else:\n",
    "        if word not in new_pos:\n",
    "            new_pos.append(word)\n",
    "\n",
    "for word in neg:\n",
    "    if word not in new_neg and word not in pos:\n",
    "        new_neg.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_pos),len(new_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add _pos to positive words\n",
    "\n",
    "new_pos_comments = []\n",
    "changed_words_pos = []\n",
    "\n",
    "for line in commentlist_pos_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_pos:\n",
    "            \n",
    "            new_word = word +\"_pos\"\n",
    "            \n",
    "#             count_changes_pos[word] += 1\n",
    "\n",
    "            new_line.append(new_word)\n",
    "            changed_words_pos.append(word)\n",
    "            \n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    new_pos_comments.append(new_line)\n",
    "\n",
    "print(new_pos_comments[:100])\n",
    "print(len(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_pos = Counter(changed_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add _neg to positive words\n",
    "\n",
    "new_neg_comments = []\n",
    "changed_words_neg = []\n",
    "count_changes_neg = 0\n",
    "\n",
    "for line in commentlist_neg_test:\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in new_neg:\n",
    "            new_word = word +\"_neg\"\n",
    "            count_changes_neg += 1\n",
    "            new_line.append(new_word)\n",
    "            changed_words_neg.append(word)\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    \n",
    "    new_neg_comments.append(new_line)\n",
    "\n",
    "print(new_neg_comments[:10])\n",
    "print(len(changed_words_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg = Counter(changed_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove double words\n",
    "words_neg = list(OrderedDict.fromkeys(changed_words_neg))\n",
    "words_pos = list(OrderedDict.fromkeys(changed_words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(words_neg, open(\"changed_words_neg_ALL.p\", \"wb\" ))\n",
    "# pickle.dump(words_pos, open(\"changed_words_pos_ALL.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_neg_comments + new_pos_comments + commentlist_neu_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of times each changed word occurs in other context for weighting the embeddings\n",
    "\n",
    "changed_word_neg_other_context = [] \n",
    "changed_word_pos_other_context = [] \n",
    "\n",
    "for line in new_data:\n",
    "    for word in line:\n",
    "        if word in words_neg:\n",
    "            changed_word_neg_other_context.append(word)\n",
    "        elif word in words_pos:\n",
    "            changed_word_pos_other_context.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_changes_neg_other = Counter(changed_word_neg_other_context)\n",
    "count_changes_pos_other = Counter(changed_word_pos_other_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightdict(count_changes_pos,count_changes_pos_other):\n",
    "    pos_word_weights = {}\n",
    "    for word in count_changes_pos:\n",
    "        pos_count = count_changes_pos[word]\n",
    "        other_count = count_changes_pos_other[word]\n",
    "        total_count_pos = pos_count + other_count\n",
    "        weight_pos = pos_count / total_count_pos\n",
    "        weight_other_pos = other_count / total_count_pos\n",
    "        pos_word_weights[word] = [weight_pos,weight_other_pos]\n",
    "    return pos_word_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_weights = getWeightdict(count_changes_pos,count_changes_pos_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word_weights = getWeightdict(count_changes_neg,count_changes_neg_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both skip models\n",
    "\n",
    "# model_SKIP = Word2Vec(new_data, size=300, sg=1)\n",
    "model_SKIP_n10 = Word2Vec(new_data, size=300, sg=1, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SKIP_n10.most_similar(\"great_pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SKIP_n10.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SKIP_n10.save(\"SKIP_negative10_EN_Change_inputSentiment_no_delete_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete other context words\n",
    "\n",
    "model_dict_delete = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        if non_word not in model_dict_delete:\n",
    "            model_dict_delete[non_word] = model_SKIP_n10.wv[word]\n",
    "        else:\n",
    "            print(word)\n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_delete:\n",
    "                model_dict_delete[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_dict_delete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dict_delete[\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_delete, open(\"SKIP_EN_Change_inputSentiment_deletedoubles.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the embeddings\n",
    "\n",
    "model_dict_average = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\":\n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            averagevector = np.average(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[averagevector]\n",
    "            \n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_average[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_average:\n",
    "                model_dict_average[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_average, open(\"SKIP_EN_Change_inputSentiment_average.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and sum embeddings\n",
    "\n",
    "model_dict_weightsum = {}\n",
    "\n",
    "count_word_del = 0\n",
    "count_all = 0\n",
    "count_else = 0\n",
    "amount_posneg = 0\n",
    "indict = 0\n",
    "\n",
    "for word in model_SKIP.wv.vocab:\n",
    "    count_all +=1\n",
    "    if word[-4:] == \"_neg\" or word[-4:] == \"_pos\": \n",
    "        amount_posneg +=1\n",
    "        non_word = word[:-4]\n",
    "        \n",
    "        if word[-4:] == \"_neg\":\n",
    "            sentiment_weight, other_weight = neg_word_weights[non_word]\n",
    "        else:\n",
    "            sentiment_weight, other_weight = pos_word_weights[non_word]\n",
    "        \n",
    "        vectorslist = []\n",
    "        posneg_vector = model_SKIP_n10.wv[word]\n",
    "        vectorslist.append(posneg_vector*sentiment_weight)\n",
    "        \n",
    "        try:\n",
    "            other_vector = model_SKIP_n10.wv[non_word]\n",
    "            vectorslist.append(other_vector*other_weight)\n",
    "            \n",
    "            vectorarray = np.array(vectorslist)\n",
    "            weight_sum_vector = np.sum(vectorarray, axis=0)\n",
    "            \n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[weight_sum_vector]\n",
    "        except:\n",
    "            # use posneg word\n",
    "            model_dict_weightsum[non_word] = model_SKIP_n10.wv[word]\n",
    "    \n",
    "    else:\n",
    "        if word not in words_neg and word not in words_pos:\n",
    "            count_else +=1\n",
    "            if word not in model_dict_weightsum:\n",
    "                model_dict_weightsum[word] = model_SKIP_n10.wv[word]\n",
    "            else:\n",
    "                print(word)\n",
    "                indict  +=1\n",
    "        else:\n",
    "            count_word_del +=1\n",
    "        \n",
    "        \n",
    "            \n",
    "print(count_all)\n",
    "print(\"\")   \n",
    "print(count_word_del)\n",
    "print(amount_posneg)\n",
    "print(count_else)\n",
    "print(\"\")\n",
    "print(count_word_del + amount_posneg + count_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict_weightsum, open(\"SKIP_EN_Change_inputSentiment_weightsum.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
