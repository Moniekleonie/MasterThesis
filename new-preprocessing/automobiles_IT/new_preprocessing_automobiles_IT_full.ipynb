{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "import json\n",
    "from keras.preprocessing import sequence as sq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id= {}\n",
    "id2word={}\n",
    "\n",
    "MAXLEN = 0\n",
    "index = 1\n",
    "\n",
    "def convertToIds(sent):\n",
    "    global index\n",
    "    global word2id\n",
    "    global id2word\n",
    "    global MAXLEN\n",
    "    ids = np.array([],dtype='int32')\n",
    "\n",
    "    if sent == None:\n",
    "        return np.append(ids,0)\n",
    "    words = sent.split()    \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in word2id:\n",
    "            ids = np.append(ids,word2id[word])\n",
    "        else:\n",
    "            if word != '':\n",
    "                #print (word, \"not in vocalbulary\")\n",
    "                word2id[word] = index\n",
    "                id2word[index] = word\n",
    "                ids = np.append(ids, index)\n",
    "                index = index + 1\n",
    "    return ids\n",
    "\n",
    "def parse(path):\n",
    "    g = open(path, 'r').read()\n",
    "    return json.loads(g)\n",
    "\n",
    "def get_annotation(data_folder,category,files,set_index):\n",
    "    global data_type\n",
    "    \n",
    "    annotation1_count = 0 # positive product\n",
    "    annotation2_count = 0 # neutral product\n",
    "    annotation3_count = 0 # negative product \n",
    "    annotation4_count = 0 # positive video\n",
    "    annotation5_count = 0 # neutral video\n",
    "    annotation6_count = 0 # negative video\n",
    "    annotation7_count = 0 # uninfo\n",
    "    \n",
    "    for file in files:\n",
    "        json_text = parse(data_folder+\"/\"+category+\"/\"+file)\n",
    "        for comment in json_text['comments']:\n",
    "            if (\"annotation\" in comment):\n",
    "                if (\"product-related\" in comment[\"annotation\"] and \"video-related\" in comment[\"annotation\"]):\n",
    "                    continue\n",
    "                if (\"positive-product\" in comment[\"annotation\"] or \"positive-video\" in comment[\"annotation\"]) and (\"negative-product\" in comment[\"annotation\"] or \"negative-video\" in comment[\"annotation\"] ):\n",
    "                    continue\n",
    "                data = [json_text['video_id'],json_text['video_description'],json_text['title'],comment['text']]\n",
    "                if  \"spam\" in comment[\"annotation\"] or \"off-topic-or-undecidable\" in comment[\"annotation\"]:\n",
    "                    data.append([0,0,0,0,0,0,1])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation7_count +=1 \n",
    "                elif \"positive-product\" in comment[\"annotation\"]:# or \"positive-video\" in comment[\"annotation\"]):\n",
    "                    data.append([1,0,0,0,0,0,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation1_count +=1\n",
    "                elif \"negative-product\" in comment[\"annotation\"]:# or \"positive-video\" in comment[\"annotation\"]):\n",
    "                    data.append([0,0,1,0,0,0,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation3_count +=1\n",
    "                elif \"product-related\" in  comment[\"annotation\"]:\n",
    "                    data.append([0,1,0,0,0,0,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation2_count +=1\n",
    "                elif \"positive-video\" in comment[\"annotation\"]:# or \"positive-video\" in comment[\"annotation\"]):\n",
    "                    data.append([0,0,0,1,0,0,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation4_count +=1\n",
    "                elif \"negative-video\" in comment[\"annotation\"]:# or \"positive-video\" in comment[\"annotation\"]):\n",
    "                    data.append([0,0,0,0,0,1,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation6_count +=1\n",
    "                elif \"video-related\" in  comment[\"annotation\"]:\n",
    "                    data.append([0,0,0,0,1,0,0])\n",
    "                    data_type[set_index].append(data)\n",
    "                    annotation5_count +=1\n",
    "    print (annotation1_count,annotation2_count,annotation3_count,annotation4_count,annotation5_count,annotation6_count,annotation7_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"automobiles_IT\"\n",
    "data_folder = \"../../SenTube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(data_folder+\"/\"+category+\"/\")]\n",
    "\n",
    "# indexes:\n",
    "# 0:trainset, 1:testset, 2:val set \n",
    "\n",
    "train_all, test = train_test_split(files,test_size=0.5,random_state=12,shuffle=True) \n",
    "train, val = train_test_split(train_all,test_size=0.2,random_state=12,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_train = []\n",
    "data_type_test = []\n",
    "data_type_val = []\n",
    "data_type = [data_type_train,data_type_test,data_type_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labeldistribution train:\")\n",
    "get_annotation(data_folder,category,train,0)\n",
    "print(\"Labeldistribution test:\")\n",
    "get_annotation(data_folder,category,test,1)\n",
    "print(\"Labeldistribution val:\")\n",
    "get_annotation(data_folder,category,val,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_type:\n",
    "    print (len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "X_val = []\n",
    "X_data = [X_train,X_test,X_val]\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_val = []\n",
    "y_data = [y_train, y_test, y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max description length : 2446\n",
    "#max title length : 80\n",
    "#max comment length: 507\n",
    "\n",
    "for i in range(0,3):\n",
    "    video_descriptions = []\n",
    "    tiles = []\n",
    "    comments = []\n",
    "    labels = []\n",
    "    for row in data_type[i]:\n",
    "        video_descriptions.append(convertToIds(row[1]))\n",
    "        tiles.append(convertToIds(row[2]))\n",
    "        comments.append(convertToIds(row[3]))\n",
    "        labels.append(row[4])\n",
    "    video_descriptions = sq.pad_sequences(video_descriptions,maxlen=2446)\n",
    "    tiles = sq.pad_sequences(tiles,maxlen=80)\n",
    "    comments = sq.pad_sequences(comments,maxlen=507)\n",
    "    X_data[i] = [video_descriptions,tiles,comments]\n",
    "    y_data[i] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump([X_data,y_data],open(\"corpus_automobiles_IT_full.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump([word2id,id2word],open(\"word2id_id2word_automobiles_IT_full.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = Glove.load(gloveFile)\n",
    "    model = {}\n",
    "    for word in f.dictionary:\n",
    "        embedding = f.word_vectors[f.dictionary[word]]\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRetrofitModel(open_model):\n",
    "    model_org = {}\n",
    "    for line in open_model:\n",
    "        line = line.rstrip()\n",
    "        line = line.split(\"\\t\")\n",
    "        try:\n",
    "            values = line[1].rstrip(\";\")\n",
    "            values = values.split(\";\")\n",
    "            if len(values) == 300:\n",
    "                model_org[line[0]] = values\n",
    "            else:\n",
    "                print(line[0], values, len(values))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return model_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding file\n",
    "\n",
    "# glove =  \"../IT_Embeddings/Italian_GloVe_embeddings.model\"\n",
    "# model_org = loadGloveModel(glove)\n",
    "\n",
    "# model_org = gensim.models.Word2Vec.load('../IT_Embeddings/home/berardi/glove_WIKI') # glove model\n",
    "# model_org = gensim.models.Word2Vec.load('../IT_Embeddings/home/common/word2vec/models/wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m') # SKIP model\n",
    "\n",
    "# model_org = gensim.models.FastText.load(\"../IT_Embeddings/FastText_default_IT.model\")\n",
    "# model_org = gensim.models.Word2Vec.load(\"../IT_Embeddings/SKIP_default_IT.model\")\n",
    "# model_org = gensim.models.Word2Vec.load(\"../IT_Embeddings/SKIP_negative10_IT.model\")\n",
    "# model_org = gensim.models.Word2Vec.load(\"../IT_Embeddings/CBOW_default_IT.model\")\n",
    "\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/model_org.p\", \"rb\"),encoding='latin1')\n",
    "\n",
    "# open_model = open('../IT_Embeddings/SKIP_default_IT_Retro_sentix.txt','r', encoding=\"utf-8\")\n",
    "# open_model = open('../IT_Embeddings/SKIP_negative10_IT_Retro_sentix.txt','r', encoding=\"utf-8\")\n",
    "# model_org = loadRetrofitModel(open_model)\n",
    "\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_IT_Change_inputSentiment_average.p\", \"rb\"))\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_IT_Change_inputSentiment_deletedoubles.p\", \"rb\"))\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_IT_Change_inputSentiment_weightsum.p\", \"rb\"))\n",
    "\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_negative10_IT_Change_inputSentiment_average.p\", \"rb\"))\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_negative10_IT_Change_inputSentiment_deletedoubles.p\", \"rb\"))\n",
    "# model_org = pickle.load(open(\"../IT_Embeddings/SKIP_negative10_IT_Change_inputSentiment_weightsum.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknowVec = np.zeros(len(model_org['tu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "\n",
    "#get W weight for embedding layer\n",
    "\n",
    "W = np.zeros(shape=(len(word2id)+1+2, dim), dtype='float32')\n",
    "W[0] = np.zeros(dim, dtype='float32')\n",
    "\n",
    "count_in = 0\n",
    "count_out = 0\n",
    "\n",
    "for word in word2id:\n",
    "    i = word2id[word]\n",
    "    if word in model_org:\n",
    "        W[i] = model_org[word]\n",
    "        count_in += 1\n",
    "    else:\n",
    "        W[i] = np.random.uniform(-0.25,0.25,dim)\n",
    "        count_out += 1\n",
    "\n",
    "print (count_in, count_out)\n",
    "print(count_in/(count_in + count_out)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump([W,word2id,id2word], open(\"automobiles_IT_full_skip_negative10_sentimentchange_weight.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
